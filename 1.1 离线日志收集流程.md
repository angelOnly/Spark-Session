## 离线日志收集流程
### 数据
数据从哪里来？
- 互联网行业：网站，app，系统(交易系统)
- 传统行业：电信，人们上网，打电话，发短信的数据

都要我们的后台去发送请求，获取数据，执行业务逻辑

app获取要展现商品数量，发送请求到后台进行交易和结账

#### 日志文件 -->(转移)--> flume agent 监控目录
1. 日志转移工具，如 linux 的 crontab 定时调度一个 shell 脚本 / python 脚本。
    
    或者自己用 java 开发一个后台服务，用 quartz 这样的框架进行定时调整，进行合并和处理。

    然后将日志文件转移到 flume agent 正在监控的目录中。

2. flume agent 启动以后，可以实时监控linux系统上的有一个目录，看其中是否有文件。
    
    只要发现有新的日志文件进来，flume 就会走后续的 channel 和 sink。通常来说 sink 都会配置为 hdfs。

3. HDFS 用来存储每天的 log 数据。因为 hadoop 可以存储大量数据。
    
    每天的日志文件可能一个 T，一天的日志文件可以存储在单机，但是1个月甚至1年的日志文件就不可能存在单机上。

4. Hive 底层也是基于 HDFS 作为数据仓库，把 HDFS 清洗后的数据导入到 Hive 表中，这里可以动态分区，Hive 使用分区表，存放每天的数据。

5. 数据仓库内部就是一些数据仓库建模的 ETL，ETL 会将原始日志所在的一个表转成几十张甚至几百张表。
    
    针对数据仓库中的表，执行临时的，或者每天定时调度的 Hive SQL ETL 作业，来进行大数据的统计和分析。

